{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "**Due:** Thursday, 2020-02-20, 11:59 PM, as a Jupyter notebook submitted via your repo in the course GitHub organization (see the detailed instructions in our LabResources repo).  Edit the provided Solutions03 notebook with your solutions.  All subproblems (or problems with no subproblems) are worth 1 point unless otherwise noted (grading will give fractional points as appropriate).\n",
    "\n",
    "In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The \"Or\" rule\n",
    "\n",
    "Recall the disjunction (logical \"or\") rule:\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B) - P(A\\land B) \\qquad ||\\;\\mathcal{C}.\n",
    "$$\n",
    "In the majority of applications, the set of propositions of interest (about either parameters or data) typically comprise an *exhaustive, mutually exclusive* set.  (*Think Bayes* author Allen Downey uses the term *suite* to denote such a set of propositions; I like that!)  For mutually exclusive propositions, $P(A\\land B) = 0$. For example, if a parameter has one value, it can't simultaneously be another value; similarly for the value of a datum. In this case, the \"or\" rule simplifies (dropping the $\\mathcal{C}$ henceforth):\n",
    "$$\n",
    "P(A\\lor B) = P(A) + P(B).\n",
    "$$\n",
    "It's simpler than the full \"or\" rule, though we've only dropped 1 term.\n",
    "\n",
    "When we derived the LTP, we used this simpler \"or\" rule, but extended to an arbitrary number of propositions, $B_i$, comprising a suite. The purpose of this exercise is to help you appreciate the impact of working with a mutually exclusive set of alternatives.  It simplifies things more than you may have realized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 1 (2 points):\n",
    "\n",
    "> Use the \"or\" and \"and\" rules to find an expression for $P(A\\lor B \\lor C)$, *not* assuming that $A$, $B$, and $C$ are mutually exclusive. Seek a result using only probabilities for the individual propositions, and \"and\" combinations of them. (You can use a comma instead of $\\land$ for \"and\" if you wish.)  You may use associativity of \"or\" and \"and;\" that is, $A\\lor (B \\lor C) \\equiv (A\\lor B) \\lor C$, and $A\\land (B \\land C) \\equiv (A\\land B) \\land C$.\n",
    "\n",
    "> Do this in two steps:\n",
    "\n",
    "> * Copy the following incomplete truth tables into your solution cell and complete them. For each table, you should find that the two bold columns have the same truth values, indicating they are logically equivalent. These tables thus establish two *rules of replacement*: where you see one of these formulas, you may substitute the other.  MathJax may make the headings hard to read, so to clarify: the first table aims to show $X\\land Y\\land X \\equiv X\\land Y$ (so you can drop repeated symbols in a multiple \"and\"), and the second table aims to show a kind of distributive rule: $X\\land (Y\\lor Z) \\equiv (X\\land Y)\\lor (X\\land Z)$.\n",
    "\n",
    "> *Hint*: You may find it easier to work on the tables using the [Markdown Tables generator - TablesGenerator.com](https://www.tablesgenerator.com/markdown_tables) web page. Note that you can copy the Markdown for a table in a notebook to your clipboard, and load it into the web page using the `File->Paste` command on the web page. Then you can revise it there, and copy the result back.\n",
    "\n",
    "| $X$ | $Y$ | ${\\bf X\\land Y}$ | ${\\bf (X\\land Y) \\land X}$ |\n",
    "|---|-----|------------|----------------------|\n",
    "| 0   | 0   |            |                      |\n",
    "| 0   | 1   |            |                      |\n",
    "| 1   | 0   |            |                      |\n",
    "| 1   | 1   |            |                      |\n",
    "\n",
    "\n",
    "| $X$ | $Y$ | $Z$ | $Y\\lor Z$ | ${\\bf X\\land (Y\\lor Z)}$  | $X\\land Y$ | $X\\land Z$ | ${\\bf (X\\land Y)\\lor (X\\land Z)}$ |\n",
    "|---|-----|-----|-----------|---------------------|------------|------------|-----------------------------|\n",
    "| 0   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 1   |           |                     |            |            |                             |\n",
    "\n",
    "> * With those replacement rules in hand, proceed with deriving the 3-proposition \"or\" rule.\n",
    "\n",
    "> *Hint*: You might guess from the 2-proposition \"or\" rule that the answer is $P(A\\lor B \\lor C) = P(A) + P(B) + P(C) - P(A\\land B \\land C)$. But that's not right. It's quite a bit more complicated (which is why having mutually exclusive alternatives is a great help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discovery chains\n",
    "\n",
    "Suppose you try to learn whether a coin flipper is biased by doing two experiments: you first count 39 heads in 100 flips, and then 55 heads in another 100 flips. From a Bayesian perspective, you could analyze these data by first finding a posterior using the 39/100 data, and then using that posterior as a prior for computing a final posterior considering in addition the 55/100 data. But you might also consider pooling the data, and just doing one calculation, computing a posterior based on seeing 94 heads in 200 flips. You might intuitively expect the two calculations will agree—and they do! This problem explores this kind of agreement.\n",
    "\n",
    "The parameter estimation problems addressed in lectures 5 through 7 involve the use of *conjugate families*: prior/likelihood pairs such that if the prior is a distribution in a particular family, and the likelihood function is built from a sampling distribution in the partner family, then the posterior distribution is another distribution from the prior's family.\n",
    "\n",
    "A handy feature of conjugate families is that inferences can be easily chained, with the posterior from inference based on an initially considered dataset, $D_1$, serving as the prior for analysis of a subsequently considered dataset, $D_2$, and producing an updated posterior that can be found by simple manipulation of the parameters in the prior family.\n",
    "\n",
    "For example, in Lec05 we considered inference of a binary outcome probability, $\\alpha$, with binomial data comprising $n$ successes in $N$ trials.  We found the posterior PDF for $\\alpha$, based on a beta PDF prior with parameters $(a,b)$:\n",
    "\\begin{align}\n",
    "p(\\alpha|n,M') \n",
    "  &\\propto \\mathop{\\mathrm{Beta}}(\\alpha|a,b) \\times \\mathop{\\mathrm{Binom}}(n|\\alpha,N)\\\\\n",
    "  &\\propto \\alpha^{a-1} (1-\\alpha)^{b-1} \\times \\alpha^{n} (1-\\alpha)^{N-n}\\\\\n",
    "  &\\propto \\alpha^{n+a-1} (1-\\alpha)^{N-n+b-1}.\n",
    "\\end{align}\n",
    "That is, the posterior is $\\mathop{\\mathrm{Beta}}(\\alpha|n+a,N-n+b)$. So schematically, the update rule, telling us how a beta prior is modified into a beta posterior, is:\n",
    "$$\n",
    "\\boxed{(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b).}\n",
    "$$\n",
    "Thus if we get additional data composed of $n'$ successes in $N'$ new trials, the final posterior PDF will again be a beta PDF, with parameters found by chaining/iterating application of the boxed update rule:\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (n+a,\\; N-n+b) \\quad\\Rightarrow\\quad (n+n'+a,\\; N+N'-n-n'+b).\n",
    "$$\n",
    "\n",
    "A simple but interesting consequence of this update rule is that we get the same overall posterior if we *pool the data*, i.e., consider a *single* binomial dataset composed of $m=n+n'$ successes in $M=N+N'$ trials.  Applying the single-stage update rule to this pooled data gives\n",
    "$$\n",
    "(a,\\; b) \\quad\\Rightarrow\\quad (m+a,\\; M-m+b) \\quad=\\quad (n+n'+a,\\; N+N'-n-n'+b),\n",
    "$$\n",
    "the same result as we found from chaining the inferences.\n",
    "\n",
    "This result is nearly trivial because, in the binomial IID sampling setting, the probability for the 2nd dataset is independent of the outcome comprising the 1st dataset, if we are given $\\alpha$ (as we are in a likelihood function—the probability for the data *given* the values of any parameters). Independence implies that the likelihood function based on the pooled data is just the product of the likelihood functions considering each dataset separately, and consistency follows easily from this.\n",
    "\n",
    "In fact, the consistency of the chained and pooled inferences is a more general result that *does not require the two datasets to have sampling distributions that are conditionally independent* (i.e., independent given the parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2",
     "points=2"
    ]
   },
   "source": [
    "### Problem 2 (2 points):\n",
    "\n",
    "> Prove the *general* consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$ (general in the sense of not assuming conditional independence).\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal (i.e., use the rules of probability theory to show that one result equals the other).\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.\n",
    "\n",
    "> *Hint:* You shouldn't have to write out any marginal likelihoods (i.e., writing them as integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Priors for multinomial inference\n",
    "\n",
    "In Lab04, we considered inference of categorical or multinomial probabilities, $\\alpha_k$ (for $k=1$ to $K$), as covered in Lec06. We used simulation from the prior (i.e., drawing samples from the prior PDF) to show that a uniform (flat, or constant) prior for the $\\alpha_k$ parameters allows for diverse categorical distributions when $K$ is small, but concentrates increasingly strongly around flat PMFs (with $\\alpha_k = 1/K$) as $K$ becomes large. In this problem, we'll look further into this issue, showing that it's a serious shortcoming, and exploring an alternative prior for this problem. A general lesson you should take away from Lab04 and this problem is that you shouldn't rely on your intuition when assigning joint priors over many parameters. In particular, the uniform prior can be dangerous when used over a large number of similar parameters.\n",
    "\n",
    "This problem will involve both analytical and computational work. The main analytical tool you'll use is the **generalized beta integral** (GBI) presented in Lec06:\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\int_0^\\infty d\\alpha_1 \\cdots \\int_0^\\infty d\\alpha_K\\;\n",
    "  \\alpha_1^{\\kappa_1-1}\\cdots\\alpha_K^{\\kappa_K-1}\\; \\delta\\left(a-\\sum_k\\alpha_k\\right) =\\\\\n",
    "    \\frac{\\Gamma(\\kappa_1)\\cdots\\Gamma(\\kappa_K)}{\\Gamma(\\kappa_0)}\\; a^{\\kappa_0-1},\n",
    "\\end{gather*}\n",
    "$$\n",
    "with $\\kappa_0 \\equiv \\sum_{k=1}^K \\kappa_k$. For the computational part, you'll be plotting samples from a Dirichlet distribution, so the code from the Lab04 Jupyter notebook on the Dirichlet distribution will be useful, including the plotting function in the `shelves.py` module (which you should import into your solutions notebook). You may copy and paste any useful code from the Lab04 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.1",
     "points=1"
    ]
   },
   "source": [
    "### Problem 3.1:\n",
    "\n",
    "In Lec06 we used a (symmetric) uniform prior for the $K$ $\\alpha_k$ parameters. Slide 10 expressed this in asymmetric form (using the first $K-1$ of the $\\alpha_k$'s):\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = \n",
    "  \\begin{cases}\n",
    "    C & \\text{for } \\alpha_k \\ge 0, \\sum_k \\alpha_k \\le 1\\\\\n",
    "    0 & \\text{otherwise.}\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "We can write this more symmetrically (i.e., using all $K$ parameters) using a Dirac $\\delta$ function:\n",
    "$$\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = C \\times \\delta\\left(1 - \\sum_k \\alpha_k\\right),\n",
    "$$\n",
    "for $\\alpha_k \\ge 0$. (For the rest of this assignment, and in your solutions, let's drop the context symbol, $\\mathcal{C}$.)\n",
    "\n",
    "Use the GBI to find the normalization constant for the uniform prior, $C$. That is, find the value of $C$ that makes the integral of the prior equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.2"
    ]
   },
   "source": [
    "### Problem 3.2 (3 points):\n",
    "\n",
    "In Lab04 we looked at samples from the symmetric uniform prior: $K$-catgory PMFs (plotted as histograms) drawn from the uniform Dirichlet distribution. Perhaps surprisingly, we found that, instead of producing a wide variety of $K$-category PMFs, the flat prior produces PMFs that increasingly concentrate around the flat PMF, $\\alpha_k = 1/K$, as $K$ gets large. In this problem we'll try to get some analytical insight into this behavior.\n",
    "\n",
    "In Lec06 (slide 15), we looked at the 3-category case ($K=3$), and used the GBI to compute the **single-category marginal posterior PDF for $\\alpha_1$**, for a posterior assuming a uniform prior (be sure to update your copy of the Lec06 slides; there was an important missing \"$+1$\" in the originally-posted version). We found that the single-category marginals are just beta distributions, like what we found for binomial inference (but not *exactly* the same). Explore this further, as follows.\n",
    "\n",
    "> * Use the GBI to find the marginal posterior PDF for $\\alpha_1$, but now with an **arbitrary** total number of categories (arbitrary $K$). That is, compute the $K-1$-dimensional integral,\n",
    "$$\n",
    "p(\\alpha_1|D) = \\int d\\alpha_2 \\int d\\alpha_3 \\cdots \\int d\\alpha_K\\; p(\\alpha_1,\\ldots,\\alpha_K|D).\n",
    "$$\n",
    "You don't need to compute the values of normalization constants; you may either introduce an arbitrary constant ($C'$, say), or use a proportionality symbol. _**Hint:**_ Note how we rearranged the arguments of the $\\delta$ function in Lec06, slide 16.\n",
    "\n",
    "> * If there are no counts ($n_k = 0$ for all $k$, so $N=0$), the posterior is just equal to the (uniform) prior. What is the marginal PDF for $\\alpha_1$ in this case?\n",
    "\n",
    "> * Recall that a beta distribution, $\\mathop{\\rm Beta}(a,b)$, with shape parameters $(a,b)$, has mean $\\mu = a/(a+b)$, and variance $\\sigma^2 = \\mu(1-\\mu)/\\gamma$, where $\\gamma\\equiv a+b+1$. Compute the mean and the *standard deviation* for the no-counts marginal you just computed. When $K$ is large ($K\\gg 1$), what is the standard deviation?\n",
    "\n",
    "> * In words, *briefly* (up to a few sentences), how do your results relate to the simulation results presented in the Lab04 Dirichlet distribution notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.3",
     "points=2"
    ]
   },
   "source": [
    "### Problem 3.3 (2 points):\n",
    "\n",
    "Multiple lines of argument suggest a good \"uninformative\" prior for multinomial inference is a symmetric Dirichlet distribution with $\\alpha_k = 1/K$:\n",
    "$$\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = \n",
    "  C \\left(\\prod_{k=1}^K \\alpha_k^{1/K -1}\\right) \\; \\delta\\left(1 - \\sum_k \\alpha_k\\right).\n",
    "$$\n",
    "We'll cover the formal arguments later, but here let's just experiment with this prior.\n",
    "\n",
    "> * Repeat the simulation exercise in the Lab04 notebook that plotted a stack of 10 PMFs sampled from the uniform prior, for $K=5$. In a cell below that, produce the same kind of plot, but with a stack of 10 PMFs drawn from this new prior.\n",
    "\n",
    "> * Do the same thing, but now for $K=40$, producing two stacks of 10 PMFs drawn from the uniform prior and the new prior for $K=40$.\n",
    "\n",
    "> * In a sentence or two, comment on how this prior behaves compared to the uniform prior."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
