{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "**Due:** Thursday, 2020-02-20, 11:59 PM, as a Jupyter notebook submitted via your repo in the course GitHub organization (see the detailed instructions in our LabResources repo).  Edit the provided Solutions03 notebook with your solutions.  All subproblems (or problems with no subproblems) are worth 1 point unless otherwise noted (grading will give fractional points as appropriate).\n",
    "\n",
    "In derivations, use a bit of text to explain your steps, but you needn't write an essay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=1",
     "points=2"
    ]
   },
   "source": [
    "### Problem 1 (2 points):\n",
    "\n",
    "> Use the \"or\" and \"and\" rules to find an expression for $P(A\\lor B \\lor C)$, *not* assuming that $A$, $B$, and $C$ are mutually exclusive. Seek a result using only probabilities for the individual propositions, and \"and\" combinations of them. (You can use a comma instead of $\\land$ for \"and\" if you wish.)  You may use associativity of \"or\" and \"and;\" that is, $A\\lor (B \\lor C) \\equiv (A\\lor B) \\lor C$, and $A\\land (B \\land C) \\equiv (A\\land B) \\land C$.\n",
    "\n",
    "> Do this in two steps:\n",
    "\n",
    "> * Copy the following incomplete truth tables into your solution cell and complete them. For each table, you should find that the two bold columns have the same truth values, indicating they are logically equivalent. These tables thus establish two *rules of replacement*: where you see one of these formulas, you may substitute the other.  MathJax may make the headings hard to read, so to clarify: the first table aims to show $X\\land Y\\land X \\equiv X\\land Y$ (so you can drop repeated symbols in a multiple \"and\"), and the second table aims to show a kind of distributive rule: $X\\land (Y\\lor Z) \\equiv (X\\land Y)\\lor (X\\land Z)$.\n",
    "\n",
    "> *Hint*: You may find it easier to work on the tables using the [Markdown Tables generator - TablesGenerator.com](https://www.tablesgenerator.com/markdown_tables) web page. Note that you can copy the Markdown for a table in a notebook to your clipboard, and load it into the web page using the `File->Paste` command on the web page. Then you can revise it there, and copy the result back.\n",
    "\n",
    "| $X$ | $Y$ | ${\\bf X\\land Y}$ | ${\\bf (X\\land Y) \\land X}$ |\n",
    "|---|-----|------------|----------------------|\n",
    "| 0   | 0   |            |                      |\n",
    "| 0   | 1   |            |                      |\n",
    "| 1   | 0   |            |                      |\n",
    "| 1   | 1   |            |                      |\n",
    "\n",
    "\n",
    "| $X$ | $Y$ | $Z$ | $Y\\lor Z$ | ${\\bf X\\land (Y\\lor Z)}$  | $X\\land Y$ | $X\\land Z$ | ${\\bf (X\\land Y)\\lor (X\\land Z)}$ |\n",
    "|---|-----|-----|-----------|---------------------|------------|------------|-----------------------------|\n",
    "| 0   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 0   | 1   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 0   | 1   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 0   |           |                     |            |            |                             |\n",
    "| 1   | 1   | 1   |           |                     |            |            |                             |\n",
    "\n",
    "> * With those replacement rules in hand, proceed with deriving the 3-proposition \"or\" rule.\n",
    "\n",
    "> *Hint*: You might guess from the 2-proposition \"or\" rule that the answer is $P(A\\lor B \\lor C) = P(A) + P(B) + P(C) - P(A\\land B \\land C)$. But that's not right. It's quite a bit more complicated (which is why having mutually exclusive alternatives is a great help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=2",
     "points=2"
    ]
   },
   "source": [
    "### Problem 2 (2 points):\n",
    "\n",
    "> Prove the *general* consistency of chained and joint inferences based on using two datasets to estimate a parameter, $\\theta$ (general in the sense of not assuming conditional independence).\n",
    "\n",
    "> 1. Use Bayes's theorem to write down the posterior PDF for $\\theta$ based on data $D_1$.\n",
    "> 2. Use the posterior from (1) as the prior for inference of $\\theta$ additionally considering new data, $D_2$, using Bayes's theorem to compute an overall posterior PDF for $\\theta$, $p(\\theta|D_1,D_2,\\mathcal{C})$. *Do not assume that the joint sampling distribution for $(D_1,D_2)$ factors*:\n",
    "$$\n",
    "p(D_1,D_2|\\theta) \\ne p(D_1|\\theta)\\times p(D_2|\\theta). \\qquad ||\\; \\mathcal{C}\n",
    "$$\n",
    "> 3. Now suppose you start with the same initial prior used in (1), but consider the two datasets together. Compute the posterior $p(\\theta|D_1,D_2,\\mathcal{C})$ in a single step, considering $(D_1,D_2)$ as a single, pooled dataset.\n",
    "> 4. Show that the results of (2) and (3) are equal (i.e., use the rules of probability theory to show that one result equals the other).\n",
    "\n",
    "> For convenience, you may drop $\\mathcal{C}$ from the notation, since the same contextual information is being used throughout.\n",
    "\n",
    "> *Hint:* You shouldn't have to write out any marginal likelihoods (i.e., writing them as integrals of prior times likelihood) in order to prove consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.1",
     "points=1"
    ]
   },
   "source": [
    "### Problem 3.1:\n",
    "\n",
    "In Lec06 we used a (symmetric) uniform prior for the $K$ $\\alpha_k$ parameters. Slide 10 expressed this in asymmetric form (using the first $K-1$ of the $\\alpha_k$'s):\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = \n",
    "  \\begin{cases}\n",
    "    C & \\text{for } \\alpha_k \\ge 0, \\sum_k \\alpha_k \\le 1\\\\\n",
    "    0 & \\text{otherwise.}\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "We can write this more symmetrically (i.e., using all $K$ parameters) using a Dirac $\\delta$ function:\n",
    "$$\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = C \\times \\delta\\left(1 - \\sum_k \\alpha_k\\right),\n",
    "$$\n",
    "for $\\alpha_k \\ge 0$. (For the rest of this assignment, and in your solutions, let's drop the context symbol, $\\mathcal{C}$.)\n",
    "\n",
    "Use the GBI to find the normalization constant for the uniform prior, $C$. That is, find the value of $C$ that makes the integral of the prior equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.2"
    ]
   },
   "source": [
    "### Problem 3.2 (3 points):\n",
    "\n",
    "In Lab04 we looked at samples from the symmetric uniform prior: $K$-catgory PMFs (plotted as histograms) drawn from the uniform Dirichlet distribution. Perhaps surprisingly, we found that, instead of producing a wide variety of $K$-category PMFs, the flat prior produces PMFs that increasingly concentrate around the flat PMF, $\\alpha_k = 1/K$, as $K$ gets large. In this problem we'll try to get some analytical insight into this behavior.\n",
    "\n",
    "In Lec06 (slide 15), we looked at the 3-category case ($K=3$), and used the GBI to compute the **single-category marginal posterior PDF for $\\alpha_1$**, for a posterior assuming a uniform prior (be sure to update your copy of the Lec06 slides; there was an important missing \"$+1$\" in the originally-posted version). We found that the single-category marginals are just beta distributions, like what we found for binomial inference (but not *exactly* the same). Explore this further, as follows.\n",
    "\n",
    "> * Use the GBI to find the marginal posterior PDF for $\\alpha_1$, but now with an **arbitrary** total number of categories (arbitrary $K$). That is, compute the $K-1$-dimensional integral,\n",
    "$$\n",
    "p(\\alpha_1|D) = \\int d\\alpha_2 \\int d\\alpha_3 \\cdots \\int d\\alpha_K\\; p(\\alpha_1,\\ldots,\\alpha_K|D).\n",
    "$$\n",
    "You don't need to compute the values of normalization constants; you may either introduce an arbitrary constant ($C'$, say), or use a proportionality symbol. _**Hint:**_ Note how we rearranged the arguments of the $\\delta$ function in Lec06, slide 16.\n",
    "\n",
    "> * If there are no counts ($n_k = 0$ for all $k$, so $N=0$), the posterior is just equal to the (uniform) prior. What is the marginal PDF for $\\alpha_1$ in this case?\n",
    "\n",
    "> * Recall that a beta distribution, $\\mathop{\\rm Beta}(a,b)$, with shape parameters $(a,b)$, has mean $\\mu = a/(a+b)$, and variance $\\sigma^2 = \\mu(1-\\mu)/\\gamma$, where $\\gamma\\equiv a+b+1$. Compute the mean and the *standard deviation* for the no-counts marginal you just computed. When $K$ is large ($K\\gg 1$), what is the standard deviation?\n",
    "\n",
    "> * In words, *briefly* (up to a few sentences), how do your results relate to the simulation results presented in the Lab04 Dirichlet distribution notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem=3.3",
     "points=2"
    ]
   },
   "source": [
    "### Problem 3.3 (2 points):\n",
    "\n",
    "Multiple lines of argument suggest a good \"uninformative\" prior for multinomial inference is a symmetric Dirichlet distribution with $\\alpha_k = 1/K$:\n",
    "$$\n",
    "p(\\alpha_1,\\ldots,\\alpha_{K-1}|\\mathcal{C}) = \n",
    "  C \\left(\\prod_{k=1}^K \\alpha_k^{1/K -1}\\right) \\; \\delta\\left(1 - \\sum_k \\alpha_k\\right).\n",
    "$$\n",
    "We'll cover the formal arguments later, but here let's just experiment with this prior.\n",
    "\n",
    "> * Repeat the simulation exercise in the Lab04 notebook that plotted a stack of 10 PMFs sampled from the uniform prior, for $K=5$. In a cell below that, produce the same kind of plot, but with a stack of 10 PMFs drawn from this new prior.\n",
    "\n",
    "> * Do the same thing, but now for $K=40$, producing two stacks of 10 PMFs drawn from the uniform prior and the new prior for $K=40$.\n",
    "\n",
    "> * In a sentence or two, comment on how this prior behaves compared to the uniform prior."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
